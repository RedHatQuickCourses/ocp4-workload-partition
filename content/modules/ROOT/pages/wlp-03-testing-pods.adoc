= 3. Testing Workload Isolation with Pods

Now that we've confirmed system components are pinned correctly, let's test how different Quality of Service (QoS) classes of pods behave under this configuration. We will deploy a CPU-intensive pod and observe its CPU usage.

## prerequire: create a namespace

. create `demo` namespace for our testing
+
[source,bash,role=execute]
----
oc new-project demo
----

== Test Case 1: BestEffort Pod

A `BestEffort` pod is created when no resource requests or limits are specified. These pods are scheduled on the isolated (workload) CPUs.

. Create the `BestEffort` pod deployment manifest:
+
[source,bash,role=execute]
----
tee $HOME/pod-besteffort.yaml << 'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-stress-deployment
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cpu-stress
  template:
    metadata:
      labels:
        app: cpu-stress
    spec:
      volumes:
        - name: temp-space
          emptyDir: {}
      containers:
        - name: stress-ng-container
          image: quay.io/wangzheng422/qimgs:centos9-test-2025.12.18.v01
          volumeMounts:
            - name: temp-space
              mountPath: "/tmp/stress-workdir"
          # No resources defined, resulting in a BestEffort QoS class
          command:
            - "/bin/bash"
            - "-c"
            - |
              echo "Starting stress test on 4 CPUs...";
              stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
EOF
----
+
. Apply the manifest:
+
[source,bash,role=execute]
----
oc apply -f $HOME/pod-besteffort.yaml
----

. Login to the pod
+
[source,bash,role=execute]
----
POD_NAME=$(oc get pods -n demo -l app=cpu-stress -o jsonpath='{.items[0].metadata.name}')

oc rsh -n demo $POD_NAME
----

. run `top`, and hit `1` to show detail of cpus
+
[source,bash,role=execute]
----
top
----

. Observe CPU usage on the host node using `top`. You will see the isolated cores (12-15) at 100% utilization.
+
.Example `top` output
[source,text]
----
%Cpu0  :  7.1 us,  3.7 sy,  0.0 ni, 86.5 id,  0.0 wa,  1.3 hi,  1.3 si,  0.0 st
%Cpu1  :  6.8 us,  3.7 sy,  0.0 ni, 87.5 id,  0.3 wa,  1.0 hi,  0.7 si,  0.0 st
%Cpu2  :  6.8 us,  4.1 sy,  0.0 ni, 87.5 id,  0.0 wa,  1.0 hi,  0.7 si,  0.0 st
%Cpu3  :  6.7 us,  4.4 sy,  0.0 ni, 87.2 id,  0.3 wa,  1.0 hi,  0.3 si,  0.0 st
%Cpu4  :  6.1 us,  3.4 sy,  0.0 ni, 89.1 id,  0.0 wa,  1.0 hi,  0.0 si,  0.3 st
%Cpu5  :  6.7 us,  5.1 sy,  0.0 ni, 86.5 id,  0.3 wa,  1.0 hi,  0.3 si,  0.0 st
%Cpu6  :  6.4 us,  3.7 sy,  0.0 ni, 88.2 id,  0.0 wa,  1.3 hi,  0.3 si,  0.0 st
%Cpu7  :  7.7 us,  3.7 sy,  0.0 ni, 86.3 id,  0.3 wa,  1.3 hi,  0.3 si,  0.3 st
%Cpu8  :  6.4 us,  4.7 sy,  0.0 ni, 87.0 id,  0.3 wa,  1.0 hi,  0.7 si,  0.0 st
%Cpu9  :  7.0 us,  3.7 sy,  0.0 ni, 87.6 id,  0.0 wa,  1.3 hi,  0.3 si,  0.0 st
%Cpu10 :  5.4 us,  4.4 sy,  0.0 ni, 88.2 id,  0.3 wa,  1.3 hi,  0.3 si,  0.0 st
%Cpu11 :  7.1 us,  2.7 sy,  0.0 ni, 88.4 id,  0.0 wa,  1.4 hi,  0.3 si,  0.0 st
%Cpu12 : 98.7 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  1.0 hi,  0.0 si,  0.0 st
%Cpu13 : 98.3 us,  0.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
%Cpu14 : 97.3 us,  1.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  1.0 hi,  0.3 si,  0.0 st
%Cpu15 : 98.0 us,  1.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
----

. Confirm the process affinity. First, find the PID of the `stress-ng` process:
+
[source,bash,role=execute]
----
ps -ef | grep "stress-ng"
----
+
.Example `ps` output
[source,text]
----
1000810+       1       0  0 14:06 ?        00:00:00 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       2       1 99 14:06 ?        00:00:27 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       3       1 99 14:06 ?        00:00:27 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       4       1 99 14:06 ?        00:00:27 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       5       1 99 14:06 ?        00:00:27 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       8       6  0 14:06 pts/0    00:00:00 grep stress-ng
----
+
. Now, check the CPU affinity of that process ID automatically:
+
[source,bash,role=execute]
----
taskset -c -p $(pgrep -o stress-ng)
----
+
.Example `taskset` output
[source,text]
----
pid 1's current affinity list: 8-15
----

. Go back to bastion node
+
[source,bash,role-execute]
----
exit
----

== Test Case 2: Burstable Pod

A `Burstable` pod has resource requests that are lower than its limits. These pods are also scheduled on the isolated CPUs.

. Update the deployment using `oc patch` to add resource requests and limits:
+
[source,bash,role=execute]
----
oc scale deployment cpu-stress-deployment --replicas=0

oc patch deployment cpu-stress-deployment -n demo --patch '
spec:
  template:
    spec:
      containers:
      - name: stress-ng-container
        resources:
          requests:
            cpu: "1"
            memory: "64Mi"
          limits:
            cpu: "2"
            memory: "128Mi"
'

oc scale deployment cpu-stress-deployment --replicas=1
----

. Login to the pod
+
[source,bash,role=execute]
----
POD_NAME=$(oc get pods -n demo -l app=cpu-stress -o jsonpath='{.items[0].metadata.name}')
oc rsh -n demo $POD_NAME
----

. Observe CPU usage on the host node using `top`. You will see the isolated cores (12-15) at 100% utilization.
+
.Example `top` output
[source,text]
----
%Cpu0  :  6.8 us,  4.4 sy,  0.0 ni, 84.4 id,  0.3 wa,  1.7 hi,  2.4 si,  0.0 st
%Cpu1  :  8.0 us,  4.3 sy,  0.0 ni, 84.6 id,  0.3 wa,  1.3 hi,  1.3 si,  0.0 st
%Cpu2  :  7.0 us,  5.4 sy,  0.0 ni, 84.9 id,  0.3 wa,  1.7 hi,  0.7 si,  0.0 st
%Cpu3  :  7.7 us,  4.7 sy,  0.0 ni, 85.3 id,  0.0 wa,  1.7 hi,  0.3 si,  0.3 st
%Cpu4  :  9.3 us,  5.3 sy,  0.0 ni, 82.7 id,  0.3 wa,  1.7 hi,  0.3 si,  0.3 st
%Cpu5  :  7.1 us,  3.7 sy,  0.0 ni, 86.8 id,  0.0 wa,  1.4 hi,  0.7 si,  0.3 st
%Cpu6  :  7.1 us,  4.7 sy,  0.0 ni, 85.8 id,  0.0 wa,  1.7 hi,  0.7 si,  0.0 st
%Cpu7  :  7.7 us,  4.7 sy,  0.0 ni, 85.5 id,  0.3 wa,  1.3 hi,  0.3 si,  0.0 st
%Cpu8  :  2.7 us,  2.0 sy,  0.0 ni, 94.7 id,  0.0 wa,  0.7 hi,  0.0 si,  0.0 st
%Cpu9  :  8.4 us,  1.0 sy,  0.0 ni, 90.0 id,  0.0 wa,  0.7 hi,  0.0 si,  0.0 st
%Cpu10 : 54.0 us,  0.0 sy,  0.0 ni, 45.0 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
%Cpu11 :  3.0 us,  1.3 sy,  0.0 ni, 95.0 id,  0.3 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu12 : 50.3 us,  0.0 sy,  0.0 ni, 49.3 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu13 : 48.3 us,  0.3 sy,  0.0 ni, 51.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu14 : 43.4 us,  0.3 sy,  0.0 ni, 56.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu15 :  2.3 us,  1.0 sy,  0.0 ni, 96.0 id,  0.0 wa,  0.7 hi,  0.0 si,  0.0 st
----

. Confirm the process affinity. First, find the PID of the `stress-ng` process:
+
[source,bash,role=execute]
----
ps -ef | grep "stress-ng"
----
+
.Example `ps` output
[source,text]
----
1000810+       1       0  0 14:04 ?        00:00:00 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       2       1 51 14:04 ?        00:00:09 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       3       1 48 14:04 ?        00:00:09 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       4       1 47 14:04 ?        00:00:09 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       5       1 49 14:04 ?        00:00:09 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       8       6  0 14:04 pts/0    00:00:00 grep stress-ng
----
+
. Now, check the CPU affinity of that process ID automatically:
+
[source,bash,role=execute]
----
taskset -c -p $(pgrep -o stress-ng)
----
+
.Example `taskset` output
[source,text]
----
pid 1's current affinity list: 8-15
----

. Go back to bastion node
+
[source,bash,role-execute]
----
exit
----

== Test Case 3: Guaranteed Pod

A `Guaranteed` pod has CPU requests equal to its CPU limits. The Kubernetes CPU Manager assigns exclusive CPUs to each container in a `Guaranteed` pod.

. Update the deployment using `oc patch` to set equal CPU requests and limits:
+
[source,bash,role=execute]
----
oc scale deployment cpu-stress-deployment --replicas=0

oc patch deployment cpu-stress-deployment -n demo --patch '
spec:
  template:
    spec:
      containers:
      - name: stress-ng-container
        resources:
          requests:
            cpu: "2"
            memory: "64Mi"
          limits:
            cpu: "2"
            memory: "64Mi"
'

oc scale deployment cpu-stress-deployment --replicas=1
----

. Login to the pod
+
[source,bash,role=execute]
----
POD_NAME=$(oc get pods -n demo -l app=cpu-stress -o jsonpath='{.items[0].metadata.name}')
oc rsh -n demo $POD_NAME
----

. Observe CPU usage on the host node using `top`. You will see the isolated cores (12-15) at 100% utilization.
+
.Example `top` output
[source,text]
----
%Cpu0  :  8.2 us,  4.8 sy,  0.0 ni, 83.0 id,  0.3 wa,  1.7 hi,  1.7 si,  0.3 st
%Cpu1  :  9.5 us,  6.1 sy,  0.0 ni, 81.3 id,  0.0 wa,  1.7 hi,  1.0 si,  0.3 st
%Cpu2  :  8.1 us,  5.8 sy,  0.0 ni, 83.4 id,  0.3 wa,  1.4 hi,  0.7 si,  0.3 st
%Cpu3  :  8.8 us,  6.5 sy,  0.0 ni, 82.3 id,  0.3 wa,  1.4 hi,  0.7 si,  0.0 st
%Cpu4  :  8.5 us,  8.1 sy,  0.0 ni, 80.7 id,  0.3 wa,  1.7 hi,  0.7 si,  0.0 st
%Cpu5  :  9.5 us,  7.8 sy,  0.0 ni, 80.3 id,  0.0 wa,  1.7 hi,  0.7 si,  0.0 st
%Cpu6  : 10.5 us,  5.8 sy,  0.0 ni, 81.6 id,  0.0 wa,  1.7 hi,  0.3 si,  0.0 st
%Cpu7  :  7.8 us,  6.8 sy,  0.0 ni, 83.0 id,  0.3 wa,  1.4 hi,  0.7 si,  0.0 st
%Cpu8  : 99.3 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  0.0 si,  0.0 st
%Cpu9  : 98.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  1.0 hi,  0.3 si,  0.0 st
%Cpu10 :  1.7 us,  0.7 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu11 :  1.7 us,  1.3 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu12 :  1.0 us,  1.0 sy,  0.0 ni, 97.7 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
%Cpu13 :  0.7 us,  1.7 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu14 :  2.0 us,  1.3 sy,  0.0 ni, 95.7 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
%Cpu15 :  0.7 us,  1.0 sy,  0.0 ni, 97.7 id,  0.0 wa,  0.3 hi,  0.3 si,  0.0 st
----

. Confirm the process affinity. First, find the PID of the `stress-ng` process:
+
[source,bash,role=execute]
----
ps -ef | grep "stress-ng"
----
+
.Example `ps` output
[source,text]
----
1000810+       1       0  0 13:59 ?        00:00:00 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       2       1 50 13:59 ?        00:00:19 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       3       1 50 13:59 ?        00:00:19 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       4       1 50 13:59 ?        00:00:19 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       5       1 50 13:59 ?        00:00:19 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
1000810+       9       6  0 14:00 pts/0    00:00:00 grep stress-ng
----
+
. Now, check the CPU affinity of that process ID automatically:
+
[source,bash,role=execute]
----
taskset -c -p $(pgrep -o stress-ng)
----
+
.Example `taskset` output
[source,text]
----
pid 1's current affinity list: 8,9
----

please note this time, it pins to 2 cpu cores.

. Go back to bastion node
+
[source,bash,role-execute]
----
exit
----