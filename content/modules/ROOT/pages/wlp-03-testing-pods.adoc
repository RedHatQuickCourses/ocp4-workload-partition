= 3. Testing Workload Isolation with Pods

Now that we've confirmed system components are pinned correctly, let's test how different Quality of Service (QoS) classes of pods behave under this configuration. We will deploy a CPU-intensive pod and observe its CPU usage.

== Test Case 1: BestEffort Pod

A `BestEffort` pod is created when no resource requests or limits are specified. These pods are scheduled on the isolated (workload) CPUs.

. Create the `BestEffort` pod deployment manifest:
+
[source,bash]
----
tee $HOME/pod-besteffort.yaml << 'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-stress-deployment
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cpu-stress
  template:
    metadata:
      labels:
        app: cpu-stress
    spec:
      volumes:
        - name: temp-space
          emptyDir: {}
      containers:
        - name: stress-ng-container
          image: quay.io/wangzheng422/qimgs:centos9-test-2025.12.18.v01
          volumeMounts:
            - name: temp-space
              mountPath: "/tmp/stress-workdir"
          # No resources defined, resulting in a BestEffort QoS class
          command:
            - "/bin/bash"
            - "-c"
            - |
              echo "Starting stress test on 4 CPUs...";
              stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
EOF
----
+
. Apply the manifest:
+
[source,bash]
----
oc apply -f $HOME/pod-besteffort.yaml
----

. Login to the ocp node
+
[source,bash]
----
NODE_NAME=$(oc get nodes -o jsonpath='{.items[0].metadata.name}') && oc debug node/$NODE_NAME
----

. Observe CPU usage on the host node using `top`. You will see the isolated cores (12-15) at 100% utilization.
+
.Example `top` output
[source,text]
----
%Cpu11 :  2.0 us,  1.3 sy,  0.0 ni, 96.3 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu12 : 98.7 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
%Cpu13 : 97.0 us,  1.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  1.3 si,  0.0 st
%Cpu14 : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu15 : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
----

. Confirm the process affinity. First, find the PID of the `stress-ng` process:
+
[source,bash]
----
ps -ef | grep "stress-ng"
----
+
.Example `ps` output
[source,text]
----
1000740+   86399   86397  0 10:57 ?        00:00:00 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
----
+
. Now, check the CPU affinity of that process ID:
+
[source,bash]
----
taskset -c -p 86399
----
+
.Example `taskset` output
[source,text]
----
pid 86399's current affinity list: 12-15
----

== Test Case 2: Burstable Pod

A `Burstable` pod has resource requests that are lower than its limits. These pods are also scheduled on the isolated CPUs.

. Create the `Burstable` pod deployment manifest:
+
[source,bash]
----
tee $HOME/pod-burstable.yaml << 'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-stress-deployment
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cpu-stress
  template:
    metadata:
      labels:
        app: cpu-stress
    spec:
      volumes:
        - name: temp-space
          emptyDir: {}
      containers:
        - name: stress-ng-container
          image: quay.io/wangzheng422/qimgs:centos9-test-2025.12.18.v01
          volumeMounts:
            - name: temp-space
              mountPath: "/tmp/stress-workdir"
          # No resources defined, resulting in a BestEffort QoS class
          command:
            - "/bin/bash"
            - "-c"
            - |
              echo "Starting stress test on 4 CPUs...";
              stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
          resources:
            requests:
              cpu: "1"
              memory: "512Mi"
            limits:
              cpu: "4"
              memory: "512Mi"
EOF
----
+
. Apply the manifest:
+
[source,bash]
----
oc apply -f $HOME/pod-burstable.yaml
----

. Login to the ocp node
+
[source,bash]
----
NODE_NAME=$(oc get nodes -o jsonpath='{.items[0].metadata.name}') && oc debug node/$NODE_NAME
----

. Observe CPU usage on the host node using `top`. You will see the isolated cores (12-15) at 100% utilization.
+
.Example `top` output
[source,text]
----
%Cpu11 :  2.0 us,  1.3 sy,  0.0 ni, 96.3 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu12 : 98.7 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
%Cpu13 : 97.0 us,  1.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  1.3 si,  0.0 st
%Cpu14 : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu15 : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
----

. Confirm the process affinity. First, find the PID of the `stress-ng` process:
+
[source,bash]
----
ps -ef | grep "stress-ng"
----
+
.Example `ps` output
[source,text]
----
1000740+   86399   86397  0 10:57 ?        00:00:00 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
----
+
. Now, check the CPU affinity of that process ID:
+
[source,bash]
----
taskset -c -p 86399
----
+
.Example `taskset` output
[source,text]
----
pid 86399's current affinity list: 12-15
----


== Test Case 3: Guaranteed Pod

A `Guaranteed` pod has CPU requests equal to its CPU limits. The Kubernetes CPU Manager assigns exclusive CPUs to each container in a `Guaranteed` pod.


. Create the `Guaranteed` pod deployment manifest:
+
[source,bash]
----
tee $HOME/pod-guaranteed.yaml << 'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-stress-deployment
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cpu-stress
  template:
    metadata:
      labels:
        app: cpu-stress
    spec:
      volumes:
        - name: temp-space
          emptyDir: {}
      containers:
        - name: stress-ng-container
          image: quay.io/wangzheng422/qimgs:centos9-test-2025.12.18.v01
          volumeMounts:
            - name: temp-space
              mountPath: "/tmp/stress-workdir"
          # No resources defined, resulting in a BestEffort QoS class
          command:
            - "/bin/bash"
            - "-c"
            - |
              echo "Starting stress test on 4 CPUs...";
              stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
          resources:
            requests:
              cpu: "3"
              memory: "512Mi"
            limits:
              cpu: "3"
              memory: "512Mi"
EOF
----
+
. Apply the manifest:
+
[source,bash]
----
oc apply -f $HOME/pod-guaranteed.yaml
----

. Login to the ocp node
+
[source,bash]
----
NODE_NAME=$(oc get nodes -o jsonpath='{.items[0].metadata.name}') && oc debug node/$NODE_NAME
----

. Observe CPU usage on the host node using `top`. You will see the isolated cores (12-15) at 100% utilization.
+
.Example `top` output
[source,text]
----
%Cpu11 :  2.0 us,  1.3 sy,  0.0 ni, 96.3 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu12 : 98.7 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  0.3 si,  0.0 st
%Cpu13 : 97.0 us,  1.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.7 hi,  1.3 si,  0.0 st
%Cpu14 : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
%Cpu15 : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st
----

. Confirm the process affinity. First, find the PID of the `stress-ng` process:
+
[source,bash]
----
ps -ef | grep "stress-ng"
----
+
.Example `ps` output
[source,text]
----
1000740+   86399   86397  0 10:57 ?        00:00:00 stress-ng --cpu 4 --cpu-load 100 --temp-path /tmp/stress-workdir
----
+
. Now, check the CPU affinity of that process ID:
+
[source,bash]
----
taskset -c -p 86399
----
+
.Example `taskset` output
[source,text]
----
pid 86399's current affinity list: 12-14
----

please note this time, it pins to 3 cpu cores.
