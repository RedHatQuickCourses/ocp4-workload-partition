= 2. Enabling, Configuring, and Verifying Workload Partitioning

Workload partitioning must be enabled during the initial cluster installation by adding the `cpuPartitioningMode: AllNodes` parameter to your `install-config.yaml` file.

.install-config.yaml
[source,yaml]
----
...
cpuPartitioningMode: AllNodes
...
----

After the cluster is installed with this setting, you can apply a `PerformanceProfile` to define the CPU allocation for different workloads.

NOTE: In this workshop environment, this configuration is performed during cluster provisioning, so you do not need to do it yourself.

== Applying the Performance Profile

In this example, we have a control plane node with 32 CPU cores. We will partition the CPUs as follows:

*   *Reserved CPUs*: The first 16 cores (0-15) are reserved for OpenShift system components and the operating system.
*   *Isolated CPUs*: The remaining 16 cores (16-31) are isolated for running user workloads.

This separation ensures that user workloads do not interfere with critical system processes.

Plesae note: this configuration is for demo purpose only, in production system, normally you reserve first 4-8 cpu cores for ocp system components and os, and all cpu core left are isolated for workload.

. Log in to the control plane node:
+
[source,bash,role=execute]
----
NODE_NAME=$(oc get nodes -o jsonpath='{.items[0].metadata.name}')

oc debug node/$NODE_NAME
----

. Check the CPU and NUMA configuration on the node:
+
[source,bash,role=execute]
----
lscpu | grep -i numa
----
+
.Example Output (for a 16-core system):
[source,text]
----
NUMA node(s):          1
NUMA node0 CPU(s):     0-31
----

. Go back to bastion node
+
[source,bash,role-execute]
----
exit
----
. Create the `PerformanceProfile` manifest:
+
[source,bash,role=execute]
----
tee $HOME/performance-profile.yaml << 'EOF'
---
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: openshift-node-performance-profile
spec:
  cpu:
    # Cores reserved for OpenShift system components and the OS
    reserved: "0-15"
    # Cores isolated for user workloads
    isolated: "16-31"
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/master: ""
  nodeSelector:
    node-role.kubernetes.io/master: ''
  numa:
    # The "restricted" policy enhances CPU affinity and memory locality
    topologyPolicy: "restricted"
  realTimeKernel:
    enabled: false
  workloadHints:
    realTime: false
    highPowerConsumption: false
    perPodPowerManagement: false
EOF
----

. Apply the manifest to the cluster:
+
[source,bash,role=execute]
----
oc apply -f $HOME/performance-profile.yaml
----

Note: After the `PerformanceProfile` is applied, the control plane nodes will reboot to apply the kernel settings. This process can take several minutes. During this time, you may temporarily lose access to the cluster and the web console.

== Verifying System Component CPU Affinity

Once the cluster is accessible again, you can verify that critical system processes, such as `etcd`, are correctly pinned to the reserved CPU cores.

. Log in to the control plane node:
+
[source,bash,role=execute]
----
NODE_NAME=$(oc get nodes -o jsonpath='{.items[0].metadata.name}') 

oc debug node/$NODE_NAME
----
. Run the following script to check the CPU affinity of `etcd` processes:
+
[source,bash,role=execute]
----
# Find all PIDs for etcd processes
ETCD_PIDS=$(ps -ef | grep "etcd " | grep -v grep | awk '{print $2}')

# Iterate through each PID and check its CPU affinity
for pid in $ETCD_PIDS; do
    echo "----------------------------------------"
    echo "Checking PID: ${pid}"

    COMMAND=$(ps -o args= -p "$pid")
    echo "Command: ${COMMAND}"

    echo -n "CPU affinity (Cpuset): "
    taskset -c -p "$pid"
done
----

. The output should confirm that the `etcd` processes are running exclusively on the reserved cores (0-11).
+
.Expected Output
[source,text]
----
----------------------------------------
Checking PID: 4332
Command: etcd --logger=zap ...
CPU affinity (Cpuset): pid 4332's current affinity list: 0-15
----------------------------------------
Checking PID: 4369
Command: etcd grpc-proxy start ...
CPU affinity (Cpuset): pid 4369's current affinity list: 0-15
----

. Go back to bastion node
+
[source,bash,role-execute]
----
exit
----